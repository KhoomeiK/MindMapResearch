subreddit,self ID,parent ID,time,text
berkeley,for53lh,t1_foqkl6r,1588006321.0,"I do agree that the generally CS culture in at Berkeley is horrible in this aspect, but I believe the reason for this is that the people who wear their achievements up their sleeves are the most loudly spoken. I have definitely ranted about this toxic community before as well, but I believe that the community at research labs are significantly better. I do recommend you look into joining one if you have not already (most people in my lab care very little about company prestige because, sincerely, it isn't all-too-impressive to us). Also, if you want want a chill guy to talk to who cares very little about such stuff, feel free to pm me. A EECS/CS major's experience at Cal does vastly depend on the community that they surround themselves in, and I do believe that if you look in the right places, you can find people as passionate and down-to-earth as you are."
berkeley,fop5ijk,t1_fooazc0,1587953532.0,Thanks for this comment. I was thinking this post misrepresented a significant group of people doing cs because they love the field.
berkeleydeeprlcourse,fnslnz6,t3_ej7gxu,1587224163.0,Maybe your loss function sign is incorrect
berkeleydeeprlcourse,fnrmr1o,t1_fnppmyo,1587194004.0,"We are using linearity of expectation. Notice that the expectation of the sum is the sum of the expectation. We are left with the sum over t of E\_{tau follows p\_theta'(tau)}\[gamma\^{t}\*A(s\_t, a\_t)\]. However, at each timestep t, we don't care about the whole trajectory--just the marginal probabilities of the state and action. Thus, this is equal to E\_{p\_theta'(s\_t, a\_t)}\[gamma\^{t}\*A(s\_t, a\_t)\]. We can turn this into E\_{p\_theta'(s\_t)}\[E\_{pi\_theta'(a\_t | s\_t)\[gamma\^{t}\*A(s\_t, a\_t)\]\] by the law of total expectation (E\[E\[X|Y\]\] = E\[X\])"
berkeleydeeprlcourse,fnp34tm,t3_g2noa4,1587138392.0,"p\_theta'(s\_t) is not the same as  p(S\_t | S\_t-1, A\_t-1) . You can think of the state marginal as the frequency in which the policy pi\_theta' will visit the states in the stationary distribution of the Markov Chain (consider constructing a probability distribution over all states and actions visited by pi\_theta' and then summing out all the actions such that w are left with states. It is the same as P(s\_t = s | pi tilde)"
berkeley,fnnx0np,t3_g2k5pl,1587104377.0,Take 188.
berkeleydeeprlcourse,fmxzbjd,t1_fa2nr67,1586474865.0,"There is a misconception here that log(f(x)) has to be concave. We know that for f concave, f(E\[X\]) >= E\[f(X)\]. In this case, we are just letting X be the random variable p(x\_i|z)p(z)/q(z)."
berkeley,fm2yml1,t1_fm2ye8e,1585708513.0,"I would advise you to PNP it (but if you do, you would have to make it up with good grades in 126, 127, etc.)"
