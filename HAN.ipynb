{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOo6sZ8ObtO6oEC9tz+vYY7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhoomeiK/MindMapResearch/blob/master/HAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-Fo7SmKR8j",
        "colab_type": "text"
      },
      "source": [
        "## DATA PREP STUFF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erpO2AIcKU68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download dataset and labels\n",
        "! pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# downloaded1 = drive.CreateFile({'id': '1oZb283stxpZn8Dn8i8e2Vh6P8d6Voj4Y'}) \n",
        "# downloaded1.GetContentFile('dataset.zip')  \n",
        "\n",
        "downloaded2 = drive.CreateFile({'id': '1-1nQU2lUwBnEyNot0EeVK72X92bdqVAu'}) \n",
        "downloaded2.GetContentFile('labels.pkl')\n",
        "\n",
        "downloaded = drive.CreateFile({'id': '1-XYU2MCNbhS8ir_7ToC5DvrUMsQy9-9E'}) \n",
        "downloaded.GetContentFile('embeddings.zip')\n",
        "\n",
        "# ! unzip dataset.zip\n",
        "# ! rm -rf cse198f_shiv/data\n",
        "# ! rm -rf cse198f_shiv/diagnostics\n",
        "# ! rm -rf cse198f_shiv/models\n",
        "# ! rm cse198f_shiv/vectors.py\n",
        "# ! ls cse198f_shiv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtLDPaK8Lx1F",
        "colab_type": "code",
        "outputId": "90b72bdf-8743-4377-9a7a-ae89ec2ba379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# read dataset into memory\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "mypath = 'cse198f_shiv'\n",
        "csvs = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "print(len(csvs))\n",
        "\n",
        "data = []\n",
        "names = []\n",
        "for csv in csvs:\n",
        "    if csv[-4:] == '.csv':\n",
        "        try:\n",
        "            data.append(pd.read_csv(join(mypath, csv), encoding='CP1252'))\n",
        "            names.append(csv[:-4])\n",
        "        except:\n",
        "            try:\n",
        "                data.append(pd.read_csv(join(mypath, csv), encoding='UTF8'))\n",
        "                names.append(csv[:-4])\n",
        "            except:\n",
        "                continue\n",
        "print(len(data))\n",
        "\n",
        "# pd.reset_option('all')\n",
        "# pd.set_option('display.max_rows', None)\n",
        "# pd.set_option('display.max_columns', None)\n",
        "# pd.set_option('display.width', None)\n",
        "# pd.set_option('display.max_colwidth', -1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2052\n",
            "1980\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwXvyU5BKmFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # download embedding tools\n",
        "# ! ls *\n",
        "# ! mkdir fastText\n",
        "# ! curl https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip > fastText/crawl-300d-2M.vec.zip\n",
        "# ! unzip fastText/crawl-300d-2M.vec.zip -d fastText/\n",
        "# ! mkdir encoder\n",
        "# ! curl -Lo encoder/infersent2.pkl https://dl.fbaipublicfiles.com/infersent/infersent2.pkl\n",
        "# ! curl https://raw.githubusercontent.com/facebookresearch/InferSent/master/models.py > models.py\n",
        "# ! mkdir embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvD3ckkkZXRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate embeddings of dataset\n",
        "import torch, os\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from absl import logging\n",
        "from models import InferSent\n",
        "\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "MODEL_PATH = 'encoder/infersent2.pkl'\n",
        "W2V_PATH = 'fastText/crawl-300d-2M.vec'\n",
        "\n",
        "def load_infersent_model(model_path=MODEL_PATH, word_embeddings_path=W2V_PATH):\n",
        "\tparams_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048, 'pool_type': 'max', 'dpout_model': 0.0, 'version': 2}\n",
        "\tinfersent = InferSent(params_model).cuda()\n",
        "\tinfersent.load_state_dict(torch.load(model_path))\n",
        "\tinfersent.set_w2v_path(word_embeddings_path)\n",
        "\tinfersent.build_vocab_k_words(K=100000)\n",
        "\treturn infersent\n",
        "\n",
        "def get_infersent_vectors(sentences, model):\n",
        "\treturn model.encode(sentences, tokenize=False, verbose=False)\n",
        "\n",
        "def get_user_data_embeddings(comments, model):\n",
        "\t# model = load_infersent_model()\n",
        "\tembedding = get_infersent_vectors(comments, model)\n",
        "\treturn embedding\n",
        "\n",
        "dataEmbeddings = []\n",
        "model = load_infersent_model()\n",
        "\n",
        "start = time.time()\n",
        "for i, name in enumerate(names):\n",
        "\tcomments = list(data[i]['text']) if 'text' in data[i] else []\n",
        "\tif len(comments) >= 1:\n",
        "\t\tprint(i, name)\n",
        "\t\ttry:\n",
        "\t\t\tembeddings = get_user_data_embeddings(data[i]['text'], model)\n",
        "\t\t\tprint(len(embeddings), 'comments')\n",
        "\t\t\tdataEmbeddings.append(embeddings)\n",
        "\t\t\twith open('embeddings/%s.pkl' % name, 'wb') as pkl:\n",
        "\t\t\t\tpickle.dump(embeddings, pkl)\n",
        "\t\texcept:\n",
        "\t\t\tprint('ERROR')\n",
        "\n",
        "print(time.time() - start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2TyG1kaKWcq",
        "colab_type": "text"
      },
      "source": [
        "## MODEL RUN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHjnF_q1FqaE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "96f2fc15-6528-4a1e-bee7-25145ce8f8b4"
      },
      "source": [
        "# load labels and user embeddings and create Users data obj\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from csv import reader\n",
        "import pickle\n",
        "from os import listdir\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class Users(Dataset):\n",
        "    def __init__(self, embedPath, labelPath): # TODO: data reading\n",
        "        self.users, self.labels = [], []\n",
        "        users = {}\n",
        "        labels = pickle.load(open(labelPath, 'rb')) # { username: [ depressionPercent, vaderScore ] }\n",
        "        for user in listdir(embedPath): # must download and unzip embeddings.zip first\n",
        "            users[user[:-4]] = pickle.load(open('%s/%s' % (embedPath, user), 'rb'))\n",
        "        \n",
        "        intersection = list(set(users.keys()).intersection(set(labels.keys())))\n",
        "        for i in intersection:\n",
        "            self.users.append(torch.tensor(users[i]))\n",
        "            self.labels.append(torch.tensor(labels[i]))\n",
        "\n",
        "        print(len(self), self[0])\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.users[i], self.labels[i][0] # remove [0]\n",
        "    \n",
        "    def __len__(self):\n",
        "        assert len(self.users) == len(self.labels)\n",
        "        return len(self.users)\n",
        "\n",
        "# ! unzip embeddings.zip\n",
        "data = DataLoader(Users('embeddings', 'labels.pkl'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1765 (tensor([[ 0.0075, -0.0307,  0.1035,  ..., -0.0002, -0.0056,  0.0229],\n",
            "        [ 0.0075, -0.0278,  0.0028,  ...,  0.0186,  0.0176, -0.0132],\n",
            "        [ 0.0075, -0.0413, -0.0464,  ..., -0.0070, -0.0356, -0.0231],\n",
            "        ...,\n",
            "        [ 0.0075, -0.0322, -0.0456,  ..., -0.0044, -0.0541, -0.0285],\n",
            "        [ 0.0075, -0.1444, -0.0628,  ..., -0.1045, -0.0613, -0.0468],\n",
            "        [ 0.0075, -0.1444, -0.0628,  ..., -0.1045, -0.0613, -0.0468]]), tensor(0.0040))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPOBU7jhQ5P5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "502cbef9-8fab-436e-d65c-886476d65162"
      },
      "source": [
        "class HAN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        batch_size=8,\n",
        "        embedding_dimension=4096, # from glove to infersent?\n",
        "        hidden_size=150, \n",
        "        n_layers=1, # multiple?\n",
        "    ):\n",
        "        super(HAN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        '''\n",
        "        GRUforward(sentences)\n",
        "        GRUbackward(sentences)\n",
        "        concat\n",
        "        attention(each sentence state) # basically a post vector\n",
        "        GRUforward(post vectors)\n",
        "        # backward?\n",
        "        attention(each post state)\n",
        "        linear output\n",
        "        '''\n",
        "\n",
        "        # self.gru1 = nn.GRU( # pass through sent vecs\n",
        "        #     embedding_dimension,\n",
        "        #     hidden_size,\n",
        "        #     num_layers=n_layers,\n",
        "        #     bidirectional=True\n",
        "        # )\n",
        "\n",
        "        # # https://www.cc.gatech.edu/~dyang888/docs/naacl16.pdf\n",
        "        # self.mlp1 = nn.Linear(hidden_size * 2, hidden_size) # attention mlp\n",
        "        # self.contextW1 = nn.Parameter(torch.Tensor(hidden_size * 2, 1)) # attention weights\n",
        "\n",
        "        self.gru2 = nn.GRU( # pass through post vecs\n",
        "            embedding_dimension,\n",
        "            hidden_size,\n",
        "            num_layers=n_layers,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        \n",
        "        self.mlp2 = nn.Linear(hidden_size * 2, hidden_size * 2) # attention mlp\n",
        "        self.contextW2 = nn.Parameter(torch.Tensor(hidden_size * 2, 1)) # attention weights\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, user):\n",
        "        # postVecs = torch.tensor([])\n",
        "        # for post in user:\n",
        "        #     sentAnnot = self.gru1(post) # tensor of sentence annotations of post\n",
        "        #     sentU = F.Tanh(self.mlp1(sentAnnot)) # hidden reps of sentAnnots\n",
        "        #     sentA = F.Softmax(sentU * self.contextW1) # att weights of sents\n",
        "        #     postVec = torch.sum(sentA * sentAnnot) # total val of post; 1 sum?\n",
        "        #     torch.cat((postVecs, postVec))\n",
        "        # print('user', user)\n",
        "\n",
        "        postAnnot, postSplitAnnot = self.gru2(user)\n",
        "        print('postAnnot & postSplitAnnot', postAnnot.shape, postSplitAnnot.shape)\n",
        "        postU = F.tanh(self.mlp2(postAnnot)) # hidden reps of postAnnots\n",
        "        print('postU & context', postU.shape, self.contextW2.shape)\n",
        "        # for u in postU[0]:\n",
        "        #     postA = F.softmax(u * self.contextW2) # att weights of posts\n",
        "        # postA = F.softmax(postU * self.contextW2) # att weights of posts\n",
        "        temp = [F.softmax(u * self.contextW2) for u in postU[0]]\n",
        "        # print(postA)\n",
        "        postA = torch.stack(temp)\n",
        "        print(postA.shape)\n",
        "        postB = torch.stack(temp)\n",
        "        print(postB.shape)\n",
        "        userVec = torch.sum(postA * postAnnot) # total val of user; 1 sum?\n",
        "\n",
        "        output = F.softmax(userVec) # classify\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  embeddings  embeddings.zip  labels.pkl  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE5ZZhfhTk_I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "f77f7eb5-7209-4415-ba56-18bb2e028e35"
      },
      "source": [
        "model = HAN().to(device)\n",
        "# data = data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = nn.MSELoss()\n",
        "losses = []\n",
        "model.train()\n",
        "\n",
        "for epoch in range(4):\n",
        "    total = 0\n",
        "    for X, Y in data:\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(X)\n",
        "        loss = criterion(pred, Y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total += 1\n",
        "        losses.append(loss.item())\n",
        "        print(loss)\n",
        "        # data.set_description(f'Loss: {loss.item():.3f}')\n",
        "    \n",
        "    epoch_loss = sum(losses) / total\n",
        "    train_losses.append(epoch_loss)\n",
        "\n",
        "    # tqdm.write(f'Epoch #{epoch + 1}\\tTrain Loss: {epoch_loss:.3f}')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "postAnnot & postSplitAnnot torch.Size([1, 100, 300]) torch.Size([2, 100, 150])\n",
            "postU & context torch.Size([1, 100, 300]) torch.Size([300, 1])\n",
            "torch.Size([100, 300, 300])\n",
            "torch.Size([100, 300, 300])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-060065564335>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-7d557b0c261b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, user)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mpostB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpostB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0muserVec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpostA\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpostAnnot\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# total val of user; 1 sum?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muserVec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# classify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (300) must match the size of tensor b (100) at non-singleton dimension 1"
          ]
        }
      ]
    }
  ]
}