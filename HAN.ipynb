{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNL2a55HuffCu/xO5Qw2vmR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhoomeiK/MindMapResearch/blob/master/HAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-Fo7SmKR8j",
        "colab_type": "text"
      },
      "source": [
        "## DATA PREP STUFF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erpO2AIcKU68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download dataset and labels\n",
        "! pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# downloaded1 = drive.CreateFile({'id': '1oZb283stxpZn8Dn8i8e2Vh6P8d6Voj4Y'}) \n",
        "# downloaded1.GetContentFile('dataset.zip')  \n",
        "\n",
        "downloaded2 = drive.CreateFile({'id': '1-1nQU2lUwBnEyNot0EeVK72X92bdqVAu'}) \n",
        "downloaded2.GetContentFile('labels.pkl')\n",
        "\n",
        "downloaded = drive.CreateFile({'id': '1-XYU2MCNbhS8ir_7ToC5DvrUMsQy9-9E'}) \n",
        "downloaded.GetContentFile('embeddings.zip')\n",
        "\n",
        "# ! unzip dataset.zip\n",
        "# ! rm -rf cse198f_shiv/data\n",
        "# ! rm -rf cse198f_shiv/diagnostics\n",
        "# ! rm -rf cse198f_shiv/models\n",
        "# ! rm cse198f_shiv/vectors.py\n",
        "# ! ls cse198f_shiv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtLDPaK8Lx1F",
        "colab_type": "code",
        "outputId": "90b72bdf-8743-4377-9a7a-ae89ec2ba379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# read dataset into memory\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "mypath = 'cse198f_shiv'\n",
        "csvs = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "print(len(csvs))\n",
        "\n",
        "data = []\n",
        "names = []\n",
        "for csv in csvs:\n",
        "    if csv[-4:] == '.csv':\n",
        "        try:\n",
        "            data.append(pd.read_csv(join(mypath, csv), encoding='CP1252'))\n",
        "            names.append(csv[:-4])\n",
        "        except:\n",
        "            try:\n",
        "                data.append(pd.read_csv(join(mypath, csv), encoding='UTF8'))\n",
        "                names.append(csv[:-4])\n",
        "            except:\n",
        "                continue\n",
        "print(len(data))\n",
        "\n",
        "# pd.reset_option('all')\n",
        "# pd.set_option('display.max_rows', None)\n",
        "# pd.set_option('display.max_columns', None)\n",
        "# pd.set_option('display.width', None)\n",
        "# pd.set_option('display.max_colwidth', -1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2052\n",
            "1980\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwXvyU5BKmFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # download embedding tools\n",
        "# ! ls *\n",
        "# ! mkdir fastText\n",
        "# ! curl https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip > fastText/crawl-300d-2M.vec.zip\n",
        "# ! unzip fastText/crawl-300d-2M.vec.zip -d fastText/\n",
        "# ! mkdir encoder\n",
        "# ! curl -Lo encoder/infersent2.pkl https://dl.fbaipublicfiles.com/infersent/infersent2.pkl\n",
        "# ! curl https://raw.githubusercontent.com/facebookresearch/InferSent/master/models.py > models.py\n",
        "# ! mkdir embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvD3ckkkZXRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate embeddings of dataset\n",
        "import torch, os\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from absl import logging\n",
        "from models import InferSent\n",
        "\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "MODEL_PATH = 'encoder/infersent2.pkl'\n",
        "W2V_PATH = 'fastText/crawl-300d-2M.vec'\n",
        "\n",
        "def load_infersent_model(model_path=MODEL_PATH, word_embeddings_path=W2V_PATH):\n",
        "\tparams_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048, 'pool_type': 'max', 'dpout_model': 0.0, 'version': 2}\n",
        "\tinfersent = InferSent(params_model).cuda()\n",
        "\tinfersent.load_state_dict(torch.load(model_path))\n",
        "\tinfersent.set_w2v_path(word_embeddings_path)\n",
        "\tinfersent.build_vocab_k_words(K=100000)\n",
        "\treturn infersent\n",
        "\n",
        "def get_infersent_vectors(sentences, model):\n",
        "\treturn model.encode(sentences, tokenize=False, verbose=False)\n",
        "\n",
        "def get_user_data_embeddings(comments, model):\n",
        "\t# model = load_infersent_model()\n",
        "\tembedding = get_infersent_vectors(comments, model)\n",
        "\treturn embedding\n",
        "\n",
        "dataEmbeddings = []\n",
        "model = load_infersent_model()\n",
        "\n",
        "start = time.time()\n",
        "for i, name in enumerate(names):\n",
        "\tcomments = list(data[i]['text']) if 'text' in data[i] else []\n",
        "\tif len(comments) >= 1:\n",
        "\t\tprint(i, name)\n",
        "\t\ttry:\n",
        "\t\t\tembeddings = get_user_data_embeddings(data[i]['text'], model)\n",
        "\t\t\tprint(len(embeddings), 'comments')\n",
        "\t\t\tdataEmbeddings.append(embeddings)\n",
        "\t\t\twith open('embeddings/%s.pkl' % name, 'wb') as pkl:\n",
        "\t\t\t\tpickle.dump(embeddings, pkl)\n",
        "\t\texcept:\n",
        "\t\t\tprint('ERROR')\n",
        "\n",
        "print(time.time() - start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2TyG1kaKWcq",
        "colab_type": "text"
      },
      "source": [
        "## MODEL RUN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHjnF_q1FqaE",
        "colab_type": "code",
        "outputId": "b9e9c30a-7def-47f6-c201-15ce9ce0786b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# load labels and user embeddings and create Users data obj\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from csv import reader\n",
        "import pickle\n",
        "from os import listdir\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class Users(Dataset):\n",
        "    def __init__(self, embedPath, labelPath): # TODO: data reading\n",
        "        self.users, self.labels = [], []\n",
        "        users = {}\n",
        "        labels = pickle.load(open(labelPath, 'rb')) # { username: [ depressionPercent, vaderScore ] }\n",
        "        for user in listdir(embedPath): # must download and unzip embeddings.zip first\n",
        "            users[user[:-4]] = pickle.load(open('%s/%s' % (embedPath, user), 'rb'))\n",
        "        \n",
        "        intersection = list(set(users.keys()).intersection(set(labels.keys())))\n",
        "        for i in intersection:\n",
        "            userTensor = torch.tensor(users[i])\n",
        "            if list(userTensor.shape)[0] != 1: # exclude single comment users\n",
        "                self.users.append(userTensor)\n",
        "                self.labels.append(torch.tensor(labels[i]))\n",
        "\n",
        "        print(len(self), self[0])\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.users[i], self.labels[i][0] # remove [0]\n",
        "    \n",
        "    def __len__(self):\n",
        "        assert len(self.users) == len(self.labels)\n",
        "        return len(self.users)\n",
        "\n",
        "# ! unzip embeddings.zip\n",
        "data = DataLoader(Users('embeddings', 'labels.pkl'))"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1733 (tensor([[ 0.0075, -0.0734,  0.1213,  ...,  0.0074,  0.0091, -0.0131],\n",
            "        [ 0.0075, -0.0401,  0.0276,  ..., -0.0014,  0.0578, -0.0513],\n",
            "        [ 0.0075, -0.0442,  0.1126,  ...,  0.0459,  0.0041, -0.0028],\n",
            "        ...,\n",
            "        [ 0.0075,  0.0237,  0.0898,  ...,  0.0257, -0.0234, -0.0166],\n",
            "        [ 0.0075, -0.0749,  0.1804,  ..., -0.0422, -0.0331, -0.0398],\n",
            "        [ 0.0075,  0.0120,  0.0301,  ...,  0.0555, -0.0101,  0.0028]]), tensor(0.0046))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPOBU7jhQ5P5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def matrix_mul(input, weight, bias=False):\n",
        "    feature_list = []\n",
        "    _input = input.squeeze(0)\n",
        "    for feature in _input:\n",
        "        feature = feature.unsqueeze(1).reshape(1, 300)\n",
        "        f = torch.mm(feature, weight)\n",
        "        if isinstance(bias, torch.nn.parameter.Parameter):\n",
        "            feature = feature + bias.expand(feature.size()[0], bias.size()[1])\n",
        "        feature = torch.tanh(feature).unsqueeze(0)\n",
        "        feature_list.append(feature)\n",
        "\n",
        "    return torch.cat(feature_list, 0).squeeze(1)\n",
        "\n",
        "def element_wise_mul(input1, input2):\n",
        "    _input1 = input1.squeeze(0)\n",
        "    feature_list = []\n",
        "    for feature_1, feature_2 in zip(_input1, input2):\n",
        "        feature = feature_1 * feature_2\n",
        "        feature_list.append(feature.unsqueeze(0))\n",
        "\n",
        "    output = torch.cat(feature_list, 0)\n",
        "    return torch.sum(output, 0).unsqueeze(0)\n",
        "\n",
        "class HAN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        batch_size=8,\n",
        "        embedding_dimension=4096, # from glove to infersent?\n",
        "        hidden_size=150, \n",
        "        n_layers=1, # multiple?\n",
        "    ):\n",
        "        super(HAN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.sent_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 2 * hidden_size))\n",
        "        self.sent_bias = nn.Parameter(torch.Tensor(1, 2 * hidden_size))\n",
        "        self.context_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 1))\n",
        "\n",
        "        self.gru = nn.GRU(embedding_dimension, hidden_size, bidirectional=True)\n",
        "        self.fc = nn.Softmax()\n",
        "        # self.sent_softmax = nn.Softmax()\n",
        "        # self.fc_softmax = nn.Softmax()\n",
        "        self._create_weights(mean=0.0, std=0.05)\n",
        "\n",
        "    def _create_weights(self, mean=0.0, std=0.05):\n",
        "        self.sent_weight.data.normal_(mean, std)\n",
        "        self.context_weight.data.normal_(mean, std)\n",
        "\n",
        "    def forward(self, user):\n",
        "        f_output, h_output = self.gru(user)\n",
        "        output = matrix_mul(f_output, self.sent_weight, self.sent_bias)\n",
        "        output = matrix_mul(output, self.context_weight).permute(1, 0)\n",
        "        output = F.softmax(output)\n",
        "        output = element_wise_mul(f_output, output.permute(1, 0)).squeeze(0)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output #, h_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE5ZZhfhTk_I",
        "colab_type": "code",
        "outputId": "81a10b2f-065e-4413-f2d9-67ff51f6f6be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "model = HAN().to(device)\n",
        "# data = data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = nn.MSELoss()\n",
        "train_losses, losses = [], []\n",
        "model.train()\n",
        "\n",
        "for epoch in range(4):\n",
        "    total = 0\n",
        "    for X, Y in data:\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(X)\n",
        "        loss = criterion(pred, Y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total += 1\n",
        "        losses.append(loss.item())\n",
        "        # data.set_description(f'Loss: {loss.item():.3f}')\n",
        "    \n",
        "    epoch_loss = sum(losses) / total\n",
        "    print(epoch_loss)\n",
        "    train_losses.append(epoch_loss)\n",
        "\n",
        "    # tqdm.write(f'Epoch #{epoch + 1}\\tTrain Loss: {epoch_loss:.3f}')"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:57: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([300])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}