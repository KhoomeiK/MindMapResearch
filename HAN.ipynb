{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOwA+sa81ijczqVuLsxTTxE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhoomeiK/MindMapResearch/blob/master/HAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-Fo7SmKR8j",
        "colab_type": "text"
      },
      "source": [
        "## DATA PREP STUFF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erpO2AIcKU68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download dataset and labels\n",
        "! pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# downloaded1 = drive.CreateFile({'id': '1oZb283stxpZn8Dn8i8e2Vh6P8d6Voj4Y'}) \n",
        "# downloaded1.GetContentFile('dataset.zip')  \n",
        "\n",
        "# ! unzip dataset.zip\n",
        "# ! rm -rf cse198f_shiv/data\n",
        "# ! rm -rf cse198f_shiv/diagnostics\n",
        "# ! rm -rf cse198f_shiv/models\n",
        "# ! rm cse198f_shiv/vectors.py\n",
        "# ! ls cse198f_shiv\n",
        "\n",
        "downloaded2 = drive.CreateFile({'id': '1-1nQU2lUwBnEyNot0EeVK72X92bdqVAu'}) \n",
        "downloaded2.GetContentFile('labels.pkl')\n",
        "\n",
        "downloaded = drive.CreateFile({'id': '1-XYU2MCNbhS8ir_7ToC5DvrUMsQy9-9E'}) \n",
        "downloaded.GetContentFile('embeddings.zip')\n",
        "\n",
        "# ! unzip embeddings.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtLDPaK8Lx1F",
        "colab_type": "code",
        "outputId": "90b72bdf-8743-4377-9a7a-ae89ec2ba379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# read dataset into memory\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "mypath = 'cse198f_shiv'\n",
        "csvs = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "print(len(csvs))\n",
        "\n",
        "data = []\n",
        "names = []\n",
        "for csv in csvs:\n",
        "    if csv[-4:] == '.csv':\n",
        "        try:\n",
        "            data.append(pd.read_csv(join(mypath, csv), encoding='CP1252'))\n",
        "            names.append(csv[:-4])\n",
        "        except:\n",
        "            try:\n",
        "                data.append(pd.read_csv(join(mypath, csv), encoding='UTF8'))\n",
        "                names.append(csv[:-4])\n",
        "            except:\n",
        "                continue\n",
        "print(len(data))\n",
        "\n",
        "# pd.reset_option('all')\n",
        "# pd.set_option('display.max_rows', None)\n",
        "# pd.set_option('display.max_columns', None)\n",
        "# pd.set_option('display.width', None)\n",
        "# pd.set_option('display.max_colwidth', -1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2052\n",
            "1980\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwXvyU5BKmFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # download embedding tools\n",
        "# ! ls *\n",
        "# ! mkdir fastText\n",
        "# ! curl https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip > fastText/crawl-300d-2M.vec.zip\n",
        "# ! unzip fastText/crawl-300d-2M.vec.zip -d fastText/\n",
        "# ! mkdir encoder\n",
        "# ! curl -Lo encoder/infersent2.pkl https://dl.fbaipublicfiles.com/infersent/infersent2.pkl\n",
        "# ! curl https://raw.githubusercontent.com/facebookresearch/InferSent/master/models.py > models.py\n",
        "# ! mkdir embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvD3ckkkZXRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate embeddings of dataset\n",
        "import torch, os\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from absl import logging\n",
        "from models import InferSent\n",
        "\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "MODEL_PATH = 'encoder/infersent2.pkl'\n",
        "W2V_PATH = 'fastText/crawl-300d-2M.vec'\n",
        "\n",
        "def load_infersent_model(model_path=MODEL_PATH, word_embeddings_path=W2V_PATH):\n",
        "\tparams_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048, 'pool_type': 'max', 'dpout_model': 0.0, 'version': 2}\n",
        "\tinfersent = InferSent(params_model).cuda()\n",
        "\tinfersent.load_state_dict(torch.load(model_path))\n",
        "\tinfersent.set_w2v_path(word_embeddings_path)\n",
        "\tinfersent.build_vocab_k_words(K=100000)\n",
        "\treturn infersent\n",
        "\n",
        "def get_infersent_vectors(sentences, model):\n",
        "\treturn model.encode(sentences, tokenize=False, verbose=False)\n",
        "\n",
        "def get_user_data_embeddings(comments, model):\n",
        "\t# model = load_infersent_model()\n",
        "\tembedding = get_infersent_vectors(comments, model)\n",
        "\treturn embedding\n",
        "\n",
        "dataEmbeddings = []\n",
        "model = load_infersent_model()\n",
        "\n",
        "start = time.time()\n",
        "for i, name in enumerate(names):\n",
        "\tcomments = list(data[i]['text']) if 'text' in data[i] else []\n",
        "\tif len(comments) >= 1:\n",
        "\t\tprint(i, name)\n",
        "\t\ttry:\n",
        "\t\t\tembeddings = get_user_data_embeddings(data[i]['text'], model)\n",
        "\t\t\tprint(len(embeddings), 'comments')\n",
        "\t\t\tdataEmbeddings.append(embeddings)\n",
        "\t\t\twith open('embeddings/%s.pkl' % name, 'wb') as pkl:\n",
        "\t\t\t\tpickle.dump(embeddings, pkl)\n",
        "\t\texcept:\n",
        "\t\t\tprint('ERROR')\n",
        "\n",
        "print(time.time() - start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2TyG1kaKWcq",
        "colab_type": "text"
      },
      "source": [
        "## MODEL RUN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHjnF_q1FqaE",
        "colab_type": "code",
        "outputId": "40c1f092-f817-49ba-f1ee-da825215f3b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "# load labels and user embeddings and create Users data obj\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from csv import reader\n",
        "import pickle\n",
        "from os import listdir\n",
        "import random\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class Users(Dataset):\n",
        "    def __init__(self, embedPath, labelPath, split): \n",
        "        self.users, self.labels = [], []\n",
        "        users = {}\n",
        "        labels = pickle.load(open(labelPath, 'rb')) # { username: [ depressionPercent, vaderScore ] }\n",
        "        fileList = listdir(embedPath)\n",
        "\n",
        "        if type(split) == float:\n",
        "            userList = random.sample(fileList, int(len(fileList) * split))\n",
        "            for user in userList: # must download and unzip embeddings.zip first\n",
        "                users[user[:-4]] = pickle.load(open('%s/%s' % (embedPath, user), 'rb'))\n",
        "            \n",
        "            intersection = list(set(users.keys()).intersection(set(labels.keys())))\n",
        "            for i in intersection:\n",
        "                userTensor = torch.tensor(users[i])\n",
        "                if list(userTensor.shape)[0] != 1: # exclude single comment users\n",
        "                    self.users.append(userTensor)\n",
        "                    self.labels.append(torch.tensor(labels[i]))\n",
        "\n",
        "            self.usernames = users.keys()\n",
        "    \n",
        "        elif type(split) == Users:\n",
        "            for user in fileList: # must download and unzip embeddings.zip first\n",
        "                if user[:-4] not in split.usernames:\n",
        "                    users[user[:-4]] = pickle.load(open('%s/%s' % (embedPath, user), 'rb'))\n",
        "            \n",
        "            intersection = list(set(users.keys()).intersection(set(labels.keys())))\n",
        "            for i in intersection:\n",
        "                userTensor = torch.tensor(users[i])\n",
        "                if list(userTensor.shape)[0] != 1: # exclude single comment users\n",
        "                    self.users.append(userTensor)\n",
        "                    self.labels.append(torch.tensor(labels[i] * 100))\n",
        "        \n",
        "        else:\n",
        "            print('ERROR')\n",
        "\n",
        "        print(len(self), self[0])\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.users[i], self.labels[i][0] * 100 # remove [0]\n",
        "    \n",
        "    def __len__(self):\n",
        "        assert len(self.users) == len(self.labels)\n",
        "        return len(self.users)\n",
        "\n",
        "def my_collate(batch):\n",
        "    data = [item[0] for item in batch]\n",
        "    target = [item[1] for item in batch]\n",
        "    target = torch.Tensor(target)\n",
        "    return [data, target]\n",
        "\n",
        "train = Users('embeddings', 'labels.pkl', 0.8)\n",
        "test = Users('embeddings', 'labels.pkl', train)\n",
        "dataloader_params = {'shuffle': True}\n",
        "# dataloader_params = {'batch_size': 4, 'shuffle': True, 'collate_fn': my_collate}\n",
        "train, test = DataLoader(train, **dataloader_params), DataLoader(test, **dataloader_params)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1387 (tensor([[ 0.0075, -0.1444, -0.0628,  ..., -0.1045, -0.0613, -0.0468],\n",
            "        [ 0.0075, -0.1444, -0.0628,  ..., -0.1045, -0.0613, -0.0468],\n",
            "        [ 0.0075, -0.0414, -0.0156,  ...,  0.0176, -0.0470,  0.0129],\n",
            "        ...,\n",
            "        [ 0.0075, -0.1234, -0.0439,  ..., -0.0574, -0.0229,  0.0077],\n",
            "        [ 0.0075, -0.1444, -0.0628,  ..., -0.1045, -0.0613, -0.0468],\n",
            "        [ 0.0075,  0.0129, -0.0454,  ...,  0.0199, -0.0122, -0.0152]]), tensor(0.4477))\n",
            "346 (tensor([[ 0.0075, -0.0605, -0.0110,  ..., -0.0185,  0.0840, -0.0117],\n",
            "        [ 0.0075,  0.1375,  0.1038,  ...,  0.0616,  0.0318,  0.0148],\n",
            "        [ 0.0075, -0.0453,  0.0519,  ...,  0.0325, -0.0033, -0.0314],\n",
            "        ...,\n",
            "        [ 0.0075, -0.0919, -0.0320,  ..., -0.0037,  0.0052, -0.0085],\n",
            "        [ 0.0075, -0.0738, -0.0455,  ..., -0.0152, -0.0375, -0.0201],\n",
            "        [ 0.0075,  0.0739,  0.1000,  ...,  0.0450,  0.0023, -0.0258]]), tensor(0.4408))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPOBU7jhQ5P5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def matrix_mul(input, weight, bias=False):\n",
        "    feature_list = []\n",
        "    _input = input.squeeze(0)\n",
        "    for feature in _input:\n",
        "        feature = feature.unsqueeze(1).reshape(1, 300)\n",
        "        feature = torch.mm(feature, weight)\n",
        "        if isinstance(bias, torch.nn.parameter.Parameter):\n",
        "            feature = feature + bias.expand(feature.size()[0], bias.size()[1])\n",
        "        feature = torch.tanh(feature).unsqueeze(0)\n",
        "        feature_list.append(feature)\n",
        "\n",
        "    return torch.cat(feature_list, 0).squeeze(1)\n",
        "\n",
        "def element_wise_mul(input1, input2):\n",
        "    _input1 = input1.squeeze(0)\n",
        "    feature_list = []\n",
        "    for feature_1, feature_2 in zip(_input1, input2):\n",
        "        feature = feature_1 * feature_2\n",
        "        feature_list.append(feature.unsqueeze(0))\n",
        "\n",
        "    output = torch.cat(feature_list, 0)\n",
        "    return torch.sum(output, 0).unsqueeze(0)\n",
        "\n",
        "class HAN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        batch_size=4,\n",
        "        embedding_dimension=4096, # from glove to infersent?\n",
        "        hidden_size=150, \n",
        "        n_layers=1, # multiple?\n",
        "    ):\n",
        "        super(HAN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.sent_weight = nn.Parameter(torch.randn(2 * hidden_size, 2 * hidden_size))\n",
        "        self.sent_bias = nn.Parameter(torch.randn(1, 2 * hidden_size))\n",
        "        self.context_weight = nn.Parameter(torch.randn(2 * hidden_size, 1))\n",
        "\n",
        "        self.gru = nn.GRU(embedding_dimension, hidden_size, bidirectional=True)\n",
        "        self.fc = nn.Linear(2 * hidden_size, 1) \n",
        "        self._create_weights(mean=0.005)\n",
        "\n",
        "    def _create_weights(self, mean=0.0, std=0.01):\n",
        "        self.sent_weight.data.normal_(mean, std)\n",
        "        self.sent_bias.data.normal_(mean, std)\n",
        "        self.context_weight.data.normal_(mean, std)\n",
        "\n",
        "    def forward(self, user):\n",
        "        f_output, h_output = self.gru(user)\n",
        "        output = matrix_mul(f_output, self.sent_weight, self.sent_bias)\n",
        "        output = matrix_mul(output, self.context_weight).permute(1, 0)\n",
        "        output = F.softmax(output)\n",
        "        output = element_wise_mul(f_output, output.permute(1, 0)).squeeze(0)\n",
        "        output = self.fc(output)\n",
        "        output = F.leaky_relu(output)\n",
        "\n",
        "        return output #, h_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE5ZZhfhTk_I",
        "colab_type": "code",
        "outputId": "6f486656-05d0-4592-e79c-0e79a2add6a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = HAN().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.MSELoss() # nn.BCELoss()\n",
        "epochs = 10\n",
        "model.train()\n",
        "\n",
        "train_losses = []\n",
        "for epoch in range(epochs):\n",
        "    total = 0\n",
        "    losses = []\n",
        "    for X, Y in train:\n",
        "        # loss = 0\n",
        "        # X, Y = [x.unsqueeze(0).to(device) for x in X], [y.to(device) for y in Y]\n",
        "        # for x, y in zip(X, Y):\n",
        "        #     pred = model(x)\n",
        "        #     # print(pred, y)\n",
        "        #     if loss == 0:\n",
        "        #         loss = criterion(pred, y)\n",
        "        #     else:\n",
        "        #         loss += criterion(pred, y)\n",
        "        # loss /= 4\n",
        "        X, Y = X.to(device), Y.to(device) # torch.FloatTensor(([0.0] if Y.item() < 0.41 else [1.0])).to(device)\n",
        "        pred = model(X)\n",
        "        loss = criterion(pred, Y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        total += 1\n",
        "\n",
        "        if total % 200 == 0:\n",
        "            print(pred.item(), Y.item(), loss.item())\n",
        "    \n",
        "    epoch_loss = sum(losses) / total\n",
        "    print(epoch, epoch_loss)\n",
        "    train_losses.append(epoch_loss)\n",
        "\n",
        "print(train_losses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.3716188967227936 0.35443854331970215 0.0002951645292341709\n",
            "0.5040928721427917 0.46450501680374146 0.0015671983128413558\n",
            "0.42707765102386475 0.3636363744735718 0.004024795722216368\n",
            "0.38586175441741943 0.465646892786026 0.006365668494254351\n",
            "0.4353763163089752 0.4037621021270752 0.0009994584834203124\n",
            "0.3826005458831787 0.24485798180103302 0.018973013386130333\n",
            "0.458107590675354 0.4326900541782379 0.0006460511358454823\n",
            "0.4417169690132141 0.46510547399520874 0.0005470221512950957\n",
            "0.351027250289917 0.359570175409317 7.298157288460061e-05\n",
            "0.3529101610183716 0.30369848012924194 0.0024217895697802305\n",
            "0.3943758010864258 0.35791105031967163 0.00132967799436301\n",
            "0.3796095848083496 0.4725065529346466 0.008629846386611462\n",
            "0.4237464666366577 0.2994723618030548 0.015444053336977959\n",
            "0.38918307423591614 0.44595348834991455 0.0032228799536824226\n",
            "0.3935449421405792 0.46333834528923035 0.004871119279414415\n",
            "0.38462355732917786 0.45765116810798645 0.005333031993359327\n",
            "0.35553690791130066 0.408041387796402 0.002756720408797264\n",
            "0.356575608253479 0.2639772593975067 0.008574454113841057\n",
            "0.41160503029823303 0.27882567048072815 0.017630359157919884\n",
            "0.34574177861213684 0.2860672175884247 0.00356105319224298\n",
            "0.38529470562934875 0.38816913962364197 8.262371011369396e-06\n",
            "0.37549084424972534 0.24805206060409546 0.016240643337368965\n",
            "0.3937033712863922 0.3965661823749542 8.195687769330107e-06\n",
            "0.3813001811504364 0.3072390556335449 0.00548505038022995\n",
            "0.47831225395202637 0.3546530306339264 0.015291603282094002\n",
            "0.41234326362609863 0.38140538334846497 0.0009571524569764733\n",
            "0.3607563078403473 0.4384836256504059 0.00604153610765934\n",
            "0 0.027862604561400557\n",
            "0.371780127286911 0.4525917172431946 0.006530513055622578\n",
            "0.48572465777397156 0.4354722797870636 0.002525301417335868\n",
            "0.46310174465179443 0.450827032327652 0.00015066856576595455\n",
            "0.39925721287727356 0.4150599241256714 0.00024972567916847765\n",
            "0.37919673323631287 0.25373393297195435 0.01574091427028179\n",
            "0.41615498065948486 0.45107534527778625 0.0012194318696856499\n",
            "0.49302107095718384 0.46458643674850464 0.0008085284498520195\n",
            "0.37003228068351746 0.3796218931674957 9.196066821459681e-05\n",
            "0.45407283306121826 0.3660446107387543 0.007748967967927456\n",
            "0.36692148447036743 0.38856831192970276 0.0004685851454269141\n",
            "0.43283262848854065 0.2560318112373352 0.03125852718949318\n",
            "0.37657150626182556 0.4500226080417633 0.005395064130425453\n",
            "0.430084764957428 0.3801296055316925 0.0024955179542303085\n",
            "0.3907022774219513 0.3558505177497864 0.00121464510448277\n",
            "0.4165384769439697 0.40467506647109985 0.00014074050704948604\n",
            "0.4231729805469513 0.46510547399520874 0.001758333994075656\n",
            "0.49959856271743774 0.4008491635322571 0.00975144375115633\n",
            "0.39775535464286804 0.3764705955982208 0.00045304096420295537\n",
            "0.38830193877220154 0.39740049839019775 8.27837866381742e-05\n",
            "0.40199026465415955 0.44515565037727356 0.001863250508904457\n",
            "0.5533485412597656 0.41661959886550903 0.01869480311870575\n",
            "0.39931291341781616 0.466564804315567 0.004522816743701696\n",
            "0.3967505097389221 0.28249815106391907 0.01305360160768032\n",
            "0.3989371061325073 0.4162740409374237 0.00030056931427679956\n",
            "0.40852901339530945 0.3793339133262634 0.0008523538708686829\n",
            "0.41214457154273987 0.4483657479286194 0.0013119736686348915\n",
            "0.40532252192497253 0.43502360582351685 0.0008821543888188899\n",
            "1 0.026424208061462652\n",
            "0.4043627083301544 0.24272538721561432 0.026126623153686523\n",
            "0.4191707372665405 0.40997979044914246 8.447350410278887e-05\n",
            "0.42216137051582336 0.4663729965686798 0.001954667968675494\n",
            "0.41282138228416443 0.2450556457042694 0.02814534306526184\n",
            "0.38980942964553833 0.2822405695915222 0.011571059934794903\n",
            "0.487800270318985 0.4089319109916687 0.006220218259841204\n",
            "0.4158942699432373 0.4424523711204529 0.0007053327281028032\n",
            "0.46198004484176636 0.4472499191761017 0.00021697660849895328\n",
            "0.49484381079673767 0.22875289618968964 0.07080437988042831\n",
            "0.34330761432647705 0.4582371711730957 0.013208802789449692\n",
            "0.3711519241333008 0.2660056948661804 0.01105572935193777\n",
            "0.3781392574310303 0.2537498474121094 0.01547272503376007\n",
            "0.3824195861816406 0.4343225061893463 0.0026939131785184145\n",
            "0.40795761346817017 0.4467957615852356 0.001508401706814766\n",
            "0.34218865633010864 0.3772186040878296 0.0012270972365513444\n",
            "0.4103787839412689 0.40993258357048035 1.9909477089186112e-07\n",
            "0.3685328960418701 0.26171863079071045 0.011409287340939045\n",
            "0.3552840054035187 0.4375636577606201 0.006769941188395023\n",
            "0.3952721953392029 0.44585615396499634 0.002558736829087138\n",
            "0.39901912212371826 0.4682786166667938 0.004796877503395081\n",
            "0.37271493673324585 0.44931885600090027 0.005868160631507635\n",
            "0.41132259368896484 0.36906561255455017 0.0017856524791568518\n",
            "0.3791780471801758 0.25757578015327454 0.014787111431360245\n",
            "0.40090709924697876 0.41447871923446655 0.00018418887339066714\n",
            "0.3421019911766052 0.4568740129470825 0.013172617182135582\n",
            "0.3843808174133301 0.4301043748855591 0.0020906436257064342\n",
            "0.39551934599876404 0.4297660291194916 0.0011728353565558791\n",
            "2 0.024899433553581443\n",
            "0.4485148787498474 0.2725914716720581 0.030949044972658157\n",
            "0.4188256561756134 0.4386594593524933 0.00039337974158115685\n",
            "0.4092567265033722 0.4451116621494293 0.0012855763779953122\n",
            "0.3689654469490051 0.2442864179611206 0.015544860623776913\n",
            "0.46372005343437195 0.40108585357666016 0.003923043143004179\n",
            "0.3897944390773773 0.26439130306243896 0.01572594605386257\n",
            "0.37231799960136414 0.42280569672584534 0.002549007534980774\n",
            "0.43283751606941223 0.46333834528923035 0.0009303005645051599\n",
            "0.389413058757782 0.476139098405838 0.007521405816078186\n",
            "0.3679552376270294 0.411255419254303 0.0018749057780951262\n",
            "0.45776528120040894 0.3906340003013611 0.004506608936935663\n",
            "0.39303988218307495 0.36980879306793213 0.0005396835040301085\n",
            "0.3822842538356781 0.4526989161968231 0.004958224482834339\n",
            "0.47104841470718384 0.4102785587310791 0.00369297550059855\n",
            "0.5187667012214661 0.39220210909843445 0.016018595546483994\n",
            "0.40881451964378357 0.4131067395210266 1.8423152141622268e-05\n",
            "0.40684762597084045 0.3532213866710663 0.0028757734689861536\n",
            "0.40997424721717834 0.4579656720161438 0.0023031767923384905\n",
            "0.32195186614990234 0.43381601572036743 0.012513588182628155\n",
            "0.41178345680236816 0.25426262617111206 0.024812811985611916\n",
            "0.38410869240760803 0.2791796326637268 0.011010107584297657\n",
            "0.4038744866847992 0.4445108473300934 0.001651313854381442\n",
            "0.3796704113483429 0.4095015525817871 0.0008898969972506166\n",
            "0.39042776823043823 0.41248708963394165 0.00048661365872249007\n",
            "0.3821357190608978 0.4776054918766022 0.009114477783441544\n",
            "0.3942541182041168 0.3727549910545349 0.00046221245429478586\n",
            "0.3827810287475586 0.2886973023414612 0.008851747959852219\n",
            "3 0.024758318584247374\n",
            "0.4190112352371216 0.4682786166667938 0.002427274826914072\n",
            "0.46235349774360657 0.46417173743247986 3.3059955057979096e-06\n",
            "0.42017918825149536 0.2976190447807312 0.015020988881587982\n",
            "0.3649221956729889 0.455725759267807 0.008245287463068962\n",
            "0.3732457160949707 0.2580474317073822 0.013270644471049309\n",
            "0.44577422738075256 0.25070351362228394 0.03805258497595787\n",
            "0.3941020369529724 0.4143208861351013 0.00040880186134018004\n",
            "0.37724781036376953 0.4115000367164612 0.0011732149869203568\n",
            "0.4811475872993469 0.6047669649124146 0.015281750820577145\n",
            "0.5547502636909485 0.41267144680023193 0.020186390727758408\n",
            "0.3507692813873291 0.27855151891708374 0.005215405020862818\n",
            "0.41970688104629517 0.4586707353591919 0.0015181819908320904\n",
            "0.4000526964664459 0.4131067395210266 0.000170408035046421\n",
            "0.4071832597255707 0.2862049341201782 0.014635755680501461\n",
            "0.4041411280632019 0.35473212599754333 0.002441249554976821\n",
            "0.3838486075401306 0.4716981053352356 0.007717534434050322\n",
            "0.41582411527633667 0.40162232518196106 0.0002016908401856199\n",
            "0.41573911905288696 0.3829551637172699 0.0010747876949608326\n",
            "0.36445745825767517 0.4074198305606842 0.0018457653932273388\n",
            "0.34153443574905396 0.25338321924209595 0.007770637050271034\n",
            "0.36829453706741333 0.40768155455589294 0.0015513371909037232\n",
            "0.4476366937160492 0.4378066062927246 9.6630617917981e-05\n",
            "0.48598712682724 0.3195488750934601 0.027701690793037415\n",
            "0.4308967888355255 0.36022287607192993 0.004994802176952362\n",
            "0.3821919560432434 0.43618643283843994 0.0029154035728424788\n",
            "0.3638650178909302 0.4344089925289154 0.004976452328264713\n",
            "0.39539310336112976 0.41009223461151123 0.0002160644653486088\n",
            "4 0.023862297100582984\n",
            "0.42573225498199463 0.39008718729019165 0.0012705709086731076\n",
            "0.38971346616744995 0.3793339133262634 0.00010773511894512922\n",
            "0.36753663420677185 0.26697924733161926 0.010111788287758827\n",
            "0.38353443145751953 0.29139745235443115 0.008489223197102547\n",
            "0.4560599625110626 0.4579228460788727 3.470335286692716e-06\n",
            "0.5007251501083374 0.2523827850818634 0.061673931777477264\n",
            "0.3828016519546509 0.4489750266075134 0.004378915298730135\n",
            "0.3931211829185486 0.30135610699653625 0.008420828729867935\n",
            "0.3636208474636078 0.25 0.012909697368741035\n",
            "0.44116443395614624 0.45165807008743286 0.00011011640162905678\n",
            "0.4649609923362732 0.44263574481010437 0.0004984166589565575\n",
            "0.5020015835762024 0.2860501706600189 0.046635013073682785\n",
            "0.4084131121635437 0.2516356408596039 0.02457917481660843\n",
            "0.390673965215683 0.45215535163879395 0.003779960796236992\n",
            "0.3884722888469696 0.44765588641166687 0.0035026981495320797\n",
            "0.3529719412326813 0.3220611810684204 0.0009554750868119299\n",
            "0.40066736936569214 0.4517629146575928 0.0026107546873390675\n",
            "0.38936713337898254 0.4326900541782379 0.001876875408925116\n",
            "0.30008465051651 0.45425450801849365 0.023768344894051552\n",
            "0.3961969017982483 0.4356638193130493 0.001557637588120997\n",
            "0.3948328495025635 0.4330542981624603 0.001460879109799862\n",
            "0.4889764189720154 0.43948137760162354 0.0024497590493410826\n",
            "0.3996128737926483 0.4539634585380554 0.002953986171633005\n",
            "0.4283021092414856 0.2509860098361969 0.03144099935889244\n",
            "0.5501293540000916 0.858152449131012 0.09487822651863098\n",
            "0.41342392563819885 0.40602096915245056 5.4803764214739203e-05\n",
            "0.4086987376213074 0.44286638498306274 0.0011674280976876616\n",
            "5 0.023556260817041073\n",
            "0.37879833579063416 0.44357505440711975 0.004196023102849722\n",
            "0.49792641401290894 0.44170939922332764 0.0031603528186678886\n",
            "0.3923199474811554 0.3756926953792572 0.0002764655218925327\n",
            "0.3921715319156647 0.2898435592651367 0.010471014305949211\n",
            "0.44582659006118774 0.4716981053352356 0.0006693353061564267\n",
            "0.3942413628101349 0.41050106287002563 0.00026437785709276795\n",
            "0.39041629433631897 0.2572559416294098 0.01773167960345745\n",
            "0.43355685472488403 0.2608433961868286 0.02982993796467781\n",
            "0.37398120760917664 0.2509860098361969 0.015127819031476974\n",
            "0.29706305265426636 0.2500588297843933 0.0022093970328569412\n",
            "0.4161599278450012 0.4483594596385956 0.0010368098737671971\n",
            "0.47198227047920227 0.4445108473300934 0.000754679087549448\n",
            "0.4133158028125763 0.4348280727863312 0.00046277776709757745\n",
            "0.40195533633232117 0.4307965636253357 0.0008318164036609232\n",
            "0.3693913519382477 0.44224393367767334 0.005307498853653669\n",
            "0.41202273964881897 0.4500221908092499 0.0014439582591876388\n",
            "0.5267685651779175 0.22875289618968964 0.08881332725286484\n",
            "0.3815983235836029 0.451060950756073 0.004825056530535221\n",
            "0.33895087242126465 0.26141291856765747 0.006012134253978729\n",
            "0.46269646286964417 0.26835814118385315 0.03776738420128822\n",
            "0.4467484951019287 0.39858385920524597 0.0023198320996016264\n",
            "0.40607789158821106 0.24572154879570007 0.02571415714919567\n",
            "0.3753683567047119 0.2733980119228363 0.010397951118648052\n",
            "0.4140748977661133 0.2523827850818634 0.02614433877170086\n",
            "0.3797208368778229 0.3839378356933594 1.7783078874344938e-05\n",
            "0.35335564613342285 0.3057463467121124 0.0022666454315185547\n",
            "0.42570963501930237 0.38813671469688416 0.0014117243699729443\n",
            "6 0.0227445582872262\n",
            "0.37507691979408264 0.36453777551651 0.00011107356112916023\n",
            "0.46123552322387695 0.43357616662979126 0.0007650399929843843\n",
            "0.38596951961517334 0.26352038979530334 0.014993789605796337\n",
            "0.4349346160888672 0.46333834528923035 0.0008067718590609729\n",
            "0.4088674783706665 0.44136130809783936 0.0010558490175753832\n",
            "0.3668745160102844 0.4175158739089966 0.0025645471177995205\n",
            "0.43412601947784424 0.38791343569755554 0.002135602990165353\n",
            "0.38474413752555847 0.45784792304039 0.005344163626432419\n",
            "0.40274226665496826 0.4648667871952057 0.0038594561628997326\n",
            "0.3732297122478485 0.25 0.015185561962425709\n",
            "0.3382989764213562 0.2927846610546112 0.002071552909910679\n",
            "0.4809221625328064 0.4305589497089386 0.0025364533066749573\n",
            "0.3977072238922119 0.3793339133262634 0.00033757853088900447\n",
            "0.5260279774665833 0.8037825226783752 0.07714758813381195\n",
            "0.381229043006897 0.4652435779571533 0.0070584421046078205\n",
            "0.4235515594482422 0.35929590463638306 0.004128789063543081\n",
            "0.30568352341651917 0.43309199810028076 0.016232918947935104\n",
            "0.4533878564834595 0.9174311757087708 0.21533620357513428\n",
            "0.42469245195388794 0.2877189517021179 0.018761739134788513\n",
            "0.44692301750183105 0.43917885422706604 5.997206608299166e-05\n",
            "0.3232613503932953 0.29139745235443115 0.0010153079638257623\n",
            "0.5395452380180359 0.4604322910308838 0.006258858367800713\n",
            "0.36667317152023315 0.4539634585380554 0.007619594223797321\n",
            "0.46685266494750977 0.43744856119155884 0.0008646013448014855\n",
            "0.4362277686595917 0.4612075686454773 0.0006239904323592782\n",
            "0.38110700249671936 0.36980879306793213 0.00012764953135047108\n",
            "0.4120404124259949 0.45303311944007874 0.001680402085185051\n",
            "7 0.021498612211493576\n",
            "0.3875521421432495 0.4071022868156433 0.00038220814894884825\n",
            "0.43574321269989014 0.40617185831069946 0.0008744649821892381\n",
            "0.32862478494644165 0.33162498474121094 9.001199032354634e-06\n",
            "0.45888862013816833 0.4693899154663086 0.00011027720029233024\n",
            "0.43395906686782837 0.45422959327697754 0.00041089425212703645\n",
            "0.4081920385360718 0.0 0.16662074625492096\n",
            "0.3912140429019928 0.36297640204429626 0.0007973643369041383\n",
            "0.4150311052799225 0.4652435779571533 0.002521292306482792\n",
            "0.4196644425392151 0.25770851969718933 0.026229720562696457\n",
            "0.4074860215187073 0.44995924830436707 0.0018039749702438712\n",
            "0.43629294633865356 0.35466742515563965 0.0066627259366214275\n",
            "0.43634936213493347 0.4660171568393707 0.0008801780641078949\n",
            "0.45171356201171875 0.23836898803710938 0.04551590606570244\n",
            "0.36487480998039246 0.3535600006580353 0.00012802491255570203\n",
            "0.48253118991851807 0.46376273036003113 0.00035225506871938705\n",
            "0.2769172191619873 0.41225114464759827 0.01831527054309845\n",
            "0.3381100296974182 0.43724551796913147 0.009827844798564911\n",
            "0.4063567817211151 0.4675028324127197 0.0037388394121080637\n",
            "0.4661160707473755 0.465646892786026 2.2012795852788258e-07\n",
            "0.4145054519176483 0.3835134506225586 0.0009605041705071926\n",
            "0.4097815155982971 0.4640844166278839 0.0029488049913197756\n",
            "0.4253419041633606 0.39653927087783813 0.0008295917068608105\n",
            "0.4146224558353424 0.4581727683544159 0.001896629692055285\n",
            "0.4164445400238037 0.47671082615852356 0.0036320253275334835\n",
            "0.40964895486831665 0.38468196988105774 0.0006233503227122128\n",
            "0.34364914894104004 0.39040929079055786 0.002186510944738984\n",
            "0.43834811449050903 0.4639284610748291 0.000654354109428823\n",
            "8 0.02017672409092896\n",
            "0.3174745738506317 0.46223247051239014 0.02095484919846058\n",
            "0.4478220045566559 0.4384836256504059 8.720532059669495e-05\n",
            "0.31231313943862915 0.4069511890411377 0.008956360630691051\n",
            "0.39975810050964355 0.4681548476219177 0.0046781147830188274\n",
            "0.3533429801464081 0.4526989161968231 0.009871602058410645\n",
            "0.38077446818351746 0.272988498210907 0.011617815122008324\n",
            "0.4001408517360687 0.41792988777160645 0.00031644979026168585\n",
            "0.3173365294933319 0.45916977524757385 0.020116670057177544\n",
            "0.44234704971313477 0.2375923991203308 0.04192446544766426\n",
            "0.5253203511238098 0.4354722797870636 0.00807267613708973\n",
            "0.36809250712394714 0.3161614239215851 0.0026968372985720634\n",
            "0.3719713091850281 0.39350613951683044 0.00046374890371225774\n",
            "0.3811567425727844 0.3031924366950989 0.006078432779759169\n",
            "0.4053390324115753 0.4518706202507019 0.00216518878005445\n",
            "0.5618163347244263 0.43725866079330444 0.015514614060521126\n",
            "0.3823675215244293 0.44578254222869873 0.0040214648470282555\n",
            "0.3618137836456299 0.44581618905067444 0.0070564039051532745\n",
            "0.4022979438304901 0.4589534401893616 0.0032098451629281044\n",
            "0.35774266719818115 0.2662510573863983 0.008370714262127876\n",
            "0.37657684087753296 0.2580815851688385 0.014041125774383545\n",
            "0.40766313672065735 0.411255419254303 1.2904493814858142e-05\n",
            "0.32067805528640747 0.28433433175086975 0.0013208662858232856\n",
            "0.3504548668861389 0.3501512110233307 9.220688212963069e-08\n",
            "0.4653882384300232 1.302931547164917 0.7014787793159485\n",
            "0.38510385155677795 0.33525750041007996 0.002484658733010292\n",
            "0.44079238176345825 0.44728320837020874 4.213082866044715e-05\n",
            "0.3970024585723877 0.3985839784145355 2.501205017324537e-06\n",
            "9 0.01911114939082914\n",
            "[0.027862604561400557, 0.026424208061462652, 0.024899433553581443, 0.024758318584247374, 0.023862297100582984, 0.023556260817041073, 0.0227445582872262, 0.021498612211493576, 0.02017672409092896, 0.01911114939082914]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5YJeO-HmXz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from matplotlib import pyplot as plt\n",
        "\n",
        "# plt.xticks(range(len(train_losses)))\n",
        "# plt.xlabel('epoch')\n",
        "# plt.ylabel('loss')\n",
        "# plt.plot(train_losses, '-ro')\n",
        "\n",
        "# torch.save(model.state_dict(), 'model_save.pkl')\n",
        "# upload = drive.CreateFile({'title': 'model_save.pkl'})\n",
        "# upload.SetContentFile('model_save.pkl')\n",
        "# upload.Upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5BcxiHTZCdC",
        "colab_type": "code",
        "outputId": "0205193f-9478-4f61-b1f9-8bbfab4d8ee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "torch.no_grad()\n",
        "total = 0\n",
        "losses = []\n",
        "\n",
        "for X, Y in test:\n",
        "    # X, Y = [x.unsqueeze(0).to(device) for x in X], [y.to(device) for y in Y]\n",
        "    # for x, y in zip(X, Y):\n",
        "    #     pred = model(x)\n",
        "    #     loss = criterion(pred, y)\n",
        "        \n",
        "    #     # print('pred', pred.item())\n",
        "    #     # print('targ', y.item())\n",
        "    #     # print('loss', loss.item())\n",
        "        \n",
        "    #     total += 1\n",
        "    #     losses.append(loss.item())\n",
        "\n",
        "    X, Y = X.to(device), Y.to(device)\n",
        "    pred = model(X)\n",
        "    loss = criterion(pred, Y)\n",
        "\n",
        "    losses.append(loss.item() if loss.item() < 1 else 0)\n",
        "    total += 1\n",
        "    \n",
        "    # if total % 20 == 0:\n",
        "    if loss.item() > 0.1:\n",
        "        print(pred.item(), Y.item(), loss.item())\n",
        "\n",
        "test_loss = sum(losses) / total\n",
        "print(test_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 59\n",
            "0.4764004349708557 0.9855071902275085 0.2591896951198578\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.609584629535675 0.23547010123729706 0.13996167480945587\n",
            "0.6329982876777649 0.2545824646949768 0.14319853484630585\n",
            "0.4782411456108093 1.1830201148986816 0.49671339988708496\n",
            "0.4327390789985657 0.8695651888847351 0.19081704318523407\n",
            "0.6160134673118591 0.21960419416427612 0.15714031457901\n",
            "0.4969024658203125 1.4568158388137817 0.921433687210083\n",
            "0.5348581671714783 1.2681158781051636 0.5376668572425842\n",
            "0.7009385824203491 0.3614157438278198 0.11527575552463531\n",
            "0.5761994123458862 3.4042551517486572 7.997900009155273\n",
            "0.4319576621055603 0.8669046759605408 0.18917889893054962\n",
            "0.02323049226526953\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}